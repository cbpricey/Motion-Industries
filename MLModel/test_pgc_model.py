# ===============================================
#  test_pgc_model.py
#  Final performance evaluation: Good vs Bad images
#  Gives you real accuracy, rejection rate, and a final score
# ===============================================

import os
import pandas as pd
import numpy as np
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModel
from joblib import load
import random
from tqdm import tqdm

# -------------------------------
# CONFIG
# -------------------------------
GOOD_EXCEL = "Capstone Good Image Cross Reference.xlsx"
BAD_EXCEL = "bad.xlsx"           # Your known bad images
IMAGE_DIR = "Output/Images"

# Model files (generated by your training script)
CLASSIFIER_PATH = "siglip_pgc_classifier.pkl"
LABEL_ENCODER_PATH = "label_encoder.pkl"
CENTROIDS_PATH = "pgc_centroids.pkl"
MODEL_NAME = "google/siglip-base-patch16-256"

# Thresholds (tune these later if needed)
VISUAL_SIM_THRESHOLD = 0.75   # Below this â†’ reject as "doesn't look like any real product"

print("Loading model and artifacts...")
processor = AutoProcessor.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME).eval()
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

clf = load(CLASSIFIER_PATH)
le = load(LABEL_ENCODER_PATH)
centroids = load(CENTROIDS_PATH)

print(f"Model loaded on {device.upper()}")
print(f"Loaded {len(centroids)} PGC centroids")

# -------------------------------
# Load good and bad data
# -------------------------------
df_good = pd.read_excel(GOOD_EXCEL)
df_bad = pd.read_excel(BAD_EXCEL)

print(f"Good images in catalog: {len(df_good)}")
print(f"Bad images (known garbage): {len(df_bad)}")

# -------------------------------
# Helper: get embedding
# -------------------------------
def get_embedding(image_path):
    if not os.path.exists(image_path):
        return None
    try:
        img = Image.open(image_path).convert("RGB")
        inputs = processor(images=img, return_tensors="pt").to(device)
        with torch.no_grad():
            emb = model.get_image_features(**inputs)
            emb = emb / emb.norm(dim=-1, keepdim=True)
        return emb.cpu().numpy().flatten()
    except:
        return None

# -------------------------------
# Test on random samples
# -------------------------------
N_GOOD = 300
N_BAD = min(300, len(df_bad))

print(f"\nTesting on {N_GOOD} random GOOD images + {N_BAD} BAD images...")

results = []

# Test GOOD images
good_sample = df_good.sample(n=N_GOOD, random_state=42)
correct = 0
for _, row in tqdm(good_sample.iterrows(), total=len(good_sample), desc="Good images"):
    path = os.path.join(IMAGE_DIR, str(row.PRIMARY_IMAGE))
    emb = get_embedding(path)
    if emb is None:
        continue
    pred_idx = clf.predict([emb])[0]
    pred_pgc = int(le.inverse_transform([pred_idx])[0])
    true_pgc = int(row.PGC)
    conf = clf.predict_proba([emb]).max()
    visual_sim = float(emb @ centroids.get(pred_pgc, np.zeros(768)))

    is_correct = (pred_pgc == true_pgc)
    is_accepted = (visual_sim >= VISUAL_SIM_THRESHOLD)
    if is_correct:
        correct += 1

    results.append({
        "type": "GOOD",
        "correct_pgc": is_correct,
        "accepted": is_accepted,
        "confidence": conf,
        "visual_sim": visual_sim,
        "pred_pgc": pred_pgc,
        "true_pgc": true_pgc
    })

# Test BAD images
bad_rejected = 0
for _, row in tqdm(df_bad.iterrows(), total=len(df_bad.sample(N_BAD)), desc="Bad images"):
    filename = str(row.PRIMARY_IMAGE)
    if pd.isna(filename):
        continue
    path = os.path.join(IMAGE_DIR, filename)
    emb = get_embedding(path)
    if emb is None:
        continue
    pred_idx = clf.predict([emb])[0]
    pred_pgc = int(le.inverse_transform([pred_idx])[0])
    visual_sim = float(emb @ centroids.get(pred_pgc, np.zeros(768)))

    is_rejected = (visual_sim < VISUAL_SIM_THRESHOLD)
    if is_rejected:
        bad_rejected += 1

    results.append({
        "type": "BAD",
        "correct_pgc": False,
        "accepted": not is_rejected,
        "confidence": 0.0,
        "visual_sim": visual_sim,
        "pred_pgc": pred_pgc,
        "true_pgc": None
    })

# -------------------------------
# FINAL SCORE
# -------------------------------
good_acc = correct / N_GOOD
bad_rejection_rate = bad_rejected / N_BAD
overall_accuracy = (correct + bad_rejected) / (N_GOOD + N_BAD)

print("\n" + "="*60)
print("           FINAL MODEL PERFORMANCE")
print("="*60)
print(f"Top-1 PGC Accuracy on Good Images:     {good_acc:.1%}  ({correct}/{N_GOOD})")
print(f"Bad Image Rejection Rate:              {bad_rejection_rate:.1%}  ({bad_rejected}/{N_BAD})")
print(f"Overall Correct Decisions:             {overall_accuracy:.1%}")
print("="*60)


# Save detailed results
pd.DataFrame(results).to_csv("model_test_results_detailed.csv", index=False)
print("Detailed results saved to model_test_results_detailed.csv")